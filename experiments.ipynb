{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcviolides/miniconda3/envs/amazon_diss/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Load the Amazon Reviews 2023 dataset for the selected domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "domains = [\"All_Beauty\", \"Video_Games\", \"Baby_Products\"]\n",
    "\n",
    "\n",
    "dfs = {}  # dictionary to store the different domains\n",
    "for domain in domains:\n",
    "    dfs[domain] = {}\n",
    "    dfs[domain][\"reviews\"] = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_review_{domain}\", trust_remote_code=True)\n",
    "    dfs[domain]['metadata'] = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", f\"raw_meta_{domain}\", split=\"full\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: All_Beauty\n",
      "Reviews:\n",
      "{'rating': 5.0, 'title': 'Such a lovely scent but not overpowering.', 'text': \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\", 'images': [], 'asin': 'B00YQ6X8EO', 'parent_asin': 'B00YQ6X8EO', 'user_id': 'AGKHLEW2SOWHNMFQIJGBECAF7INQ', 'timestamp': 1588687728923, 'helpful_vote': 0, 'verified_purchase': True}\n",
      "Metadata:\n",
      "{'main_category': 'All Beauty', 'title': 'Howard LC0008 Leather Conditioner, 8-Ounce (4-Pack)', 'average_rating': 4.8, 'rating_number': 10, 'features': [], 'description': [], 'price': 'None', 'images': {'hi_res': [None, 'https://m.media-amazon.com/images/I/71i77AuI9xL._SL1500_.jpg'], 'large': ['https://m.media-amazon.com/images/I/41qfjSfqNyL.jpg', 'https://m.media-amazon.com/images/I/41w2yznfuZL.jpg'], 'thumb': ['https://m.media-amazon.com/images/I/41qfjSfqNyL._SS40_.jpg', 'https://m.media-amazon.com/images/I/41w2yznfuZL._SS40_.jpg'], 'variant': ['MAIN', 'PT01']}, 'videos': {'title': [], 'url': [], 'user_id': []}, 'store': 'Howard Products', 'categories': [], 'details': '{\"Package Dimensions\": \"7.1 x 5.5 x 3 inches; 2.38 Pounds\", \"UPC\": \"617390882781\"}', 'parent_asin': 'B01CUPMQZE', 'bought_together': None, 'subtitle': None, 'author': None}\n",
      "\n",
      "Domain: Video_Games\n",
      "Reviews:\n",
      "{'rating': 4.0, 'title': 'It’s pretty sexual. Not my fav', 'text': 'I’m playing on ps5 and it’s interesting.  It’s unique, massive, and has a neat story.  People are freaking out angry about this game.  I don’t think it’s a top 10 game but it’s definitely a good game on ps5 (played at launch).', 'images': [], 'asin': 'B07DJWBYKP', 'parent_asin': 'B07DK1H3H5', 'user_id': 'AGCI7FAH4GL5FI65HYLKWTMFZ2CQ', 'timestamp': 1608186804795, 'helpful_vote': 0, 'verified_purchase': True}\n",
      "Metadata:\n",
      "{'main_category': 'Video Games', 'title': 'Dash 8-300 Professional Add-On', 'average_rating': 5.0, 'rating_number': 1, 'features': [\"Features Dash 8-300 and 8-Q300 ('Q' rollout livery)\", 'Airlines - US Airways, South African Express, Bahamasair, Augsburg Airways, Lufthansa Cityline, British Airways (Union Jack), British European, FlyBe, Intersky, Wideroe, Iberia, Tyrolean, QantasLink, BWIA', 'Airports include - London City, Frankfurt, Milan and Amsterdam Schipol', 'Includes PSS PanelConfig and LoadEdit tools'], 'description': ['The Dash 8-300 Professional Add-On lets you pilot a real commuter special. Fly two versions of the popular Dash 8-300 in a total of 17 different liveries. The Dash 8-300 is one of the most popular short-haul aircraft available and this superbly modelled version from acclaimed aircraft developers PSS is modelled in two versions with a total of 17 different liveries. The package also includes scenery for three European airports, tutorials, tutorial flights and utilities together in one fantastic package.'], 'price': 'None', 'images': {'hi_res': [None], 'large': ['https://m.media-amazon.com/images/I/21DVWE41A0L.jpg'], 'thumb': ['https://m.media-amazon.com/images/I/21DVWE41A0L._SX38_SY50_CR,0,0,38,50_.jpg'], 'variant': ['MAIN']}, 'videos': {'title': [], 'url': [], 'user_id': []}, 'store': 'Aerosoft', 'categories': ['Video Games', 'PC', 'Games'], 'details': '{\"Pricing\": \"The strikethrough price is the List Price. Savings represents a discount off the List Price.\", \"Package Dimensions\": \"7.5 x 5.5 x 0.6 inches; 4.8 Ounces\", \"Type of item\": \"CD-ROM\", \"Rated\": \"Everyone\", \"Item Weight\": \"4.8 ounces\", \"Manufacturer\": \"Aerosoft N.A. LTD\", \"Date First Available\": \"October 2, 2001\"}', 'parent_asin': 'B000FH0MHO', 'bought_together': None, 'subtitle': None, 'author': None}\n",
      "\n",
      "Domain: Baby_Products\n",
      "Reviews:\n",
      "{'rating': 4.0, 'title': 'Good buy for preschool naps and home use...', 'text': 'I bought two of these for my kids for nap time at their preschool. But they also use at home to &#34;camp&#34;...lol. They seem to work pretty well are cute designs and have held up through multiple washings on the gentle cycle and air fluff or lay flat to dry. They still look brand new to me! They are a little stiff at first ( and sometimes after washing) but with use they do become softer and the inside is plenty soft for sleeping- my kiddos have not complaints!', 'images': [], 'asin': 'B004FM7VOW', 'parent_asin': 'B089MS68G8', 'user_id': 'AGKASBHYZPGTEPO6LWZPVJWB2BVA', 'timestamp': 1471546337000, 'helpful_vote': 1, 'verified_purchase': True}\n",
      "Metadata:\n",
      "{'main_category': 'Baby', 'title': 'Chicco Viaro Travel System, Teak', 'average_rating': 4.6, 'rating_number': 125, 'features': ['Aluminum', 'Imported', 'Convenient one-hand quick fold. Assembled Dimensions- 38 x 25.5 x 41.25 inches. Folded Dimensions- 13.5 x 25.5 x 33.25 inches. Product Weight- 18 pounds', 'Ultra-light weight aluminum frame', '3 wheel design allows for nimble steering and a sporty stance', 'Front wheel diameter 7 inches and rear wheel diameter 8.75 inches'], 'description': ['Product Description', 'For ultimate convenience, the Chicco Viaro Quick-Fold Stroller has a sleek three-wheel design, lightweight aluminum frame, and one-hand quick fold. A pull-strap and button are conveniently tucked under the seat and easy to activate simultaneously for a compact, free-standing fold. The stroller is even easier to open again after closing.For infants, the Viaro Stroller functions as a travel system with easy click-in attachment for the KeyFit 30 Infant Car Seat. For older riders, the Viaro Stroller includes a detachable tray with two cup holders, adjustable canopy, and multi-position backrest. A swiveling front wheel and suspension help maintain a smooth ride from surface to surface. Toe-tap rear brakes keep the stroller in place when parked. For parents, the Viaro Stroller features a padded push-handle, parent tray with two cup holders, and a large basket that is easily accessible from the front or back.  The Viaro Travel System includes the #1-rated Chicco KeyFit 30 Infant Car Seat, which accommodates infants from 4 to 30 lbs and up to 30\". The KeyFit 30 is the easiest infant car seat to install simply, accurately, and securely every time.• Includes the #1-rated Chicco KeyFit 30 Infant Car Seat• Lightweight aluminum frame and sleek 3-wheel design• One-hand, free-standing fold • Multi-position reclining seat and adjustable canopy • Child tray with two cup holders• Parent tray with two cup holders • Swiveling front wheel with suspension for maneuverability • Toe-tap rear brakes for parking• Large storage basket with front and rear access• Padded handle', 'Brand Story', 'By Chicco'], 'price': 'None', 'images': {'hi_res': ['https://m.media-amazon.com/images/I/91-YubHENxL._SL1500_.jpg', 'https://m.media-amazon.com/images/I/91b1hR5R+rL._SL1500_.jpg', 'https://m.media-amazon.com/images/I/A1D3XUfALZL._SL1500_.jpg', 'https://m.media-amazon.com/images/I/916+blcO2xL._SL1500_.jpg', 'https://m.media-amazon.com/images/I/91lFOTDFZ7L._SL1500_.jpg'], 'large': ['https://m.media-amazon.com/images/I/51fWzrTOFjL.jpg', 'https://m.media-amazon.com/images/I/51Z0+Mb6uOL.jpg', 'https://m.media-amazon.com/images/I/513j2XBNBIL.jpg', 'https://m.media-amazon.com/images/I/61FLwa7fhYL.jpg', 'https://m.media-amazon.com/images/I/51On5-Uga-L.jpg'], 'thumb': ['https://m.media-amazon.com/images/I/51fWzrTOFjL._SS40_.jpg', 'https://m.media-amazon.com/images/I/51Z0+Mb6uOL._SS40_.jpg', 'https://m.media-amazon.com/images/I/513j2XBNBIL._SS40_.jpg', 'https://m.media-amazon.com/images/I/61FLwa7fhYL._SS40_.jpg', 'https://m.media-amazon.com/images/I/51On5-Uga-L._SS40_.jpg'], 'variant': ['MAIN', 'PT01', 'PT02', 'PT03', 'PT04']}, 'videos': {'title': ['Viaro Demo Video', 'Chicco Viaro Video'], 'url': ['https://www.amazon.com/vdp/49e4f1f5ce1c4938b4c5737864a7d8f8?ref=dp_vse_rvc_0', 'https://www.amazon.com/vdp/5baa3f176f4c4faba3479726096477ab?ref=dp_vse_rvc_1'], 'user_id': ['', '']}, 'store': 'Chicco', 'categories': ['Baby Products', 'Strollers & Accessories', 'Strollers', 'Travel Systems'], 'details': '{\"Product Dimensions\": \"38 x 41.25 x 25.5 inches\", \"Item model number\": \"06079747300070\", \"Is Discontinued By Manufacturer\": \"No\", \"Target gender\": \"Unisex\", \"Minimum weight recommendation\": \"4 Pounds\", \"Maximum weight recommendation\": \"50 Pounds\", \"Material Type\": \"Aluminum\", \"Number Of Items\": \"1\", \"Style\": \"Teak\", \"Batteries required\": \"No\", \"Harness type\": \"5 Point\", \"Item Weight\": \"18 pounds\", \"Brand\": \"Chicco\", \"Color\": \"Teak\", \"Material\": \"Aluminum\", \"Fabric Type\": \"Aluminum\", \"Frame Material\": \"Aluminum\"}', 'parent_asin': 'B01C4319LO', 'bought_together': None, 'subtitle': None, 'author': None}\n",
      "\n",
      "Domain: All_Beauty\n",
      "Dataset size: 701528\n",
      "Domain: Video_Games\n",
      "Dataset size: 4624615\n",
      "Domain: Baby_Products\n",
      "Dataset size: 6028884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 112590/112590 [00:03<00:00, 28456.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: All_Beauty\n",
      "Items with images: 112590\n",
      "Total items: 112590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 137269/137269 [00:05<00:00, 23033.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Video_Games\n",
      "Items with images: 137269\n",
      "Total items: 137269\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 217724/217724 [00:09<00:00, 22835.49 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Baby_Products\n",
      "Items with images: 217724\n",
      "Total items: 217724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 112590/112590 [00:03<00:00, 31777.25 examples/s]\n",
      "Filter: 100%|██████████| 701528/701528 [00:04<00:00, 152402.66 examples/s]\n",
      "Filter: 100%|██████████| 137269/137269 [00:05<00:00, 25977.19 examples/s]\n",
      "Filter: 100%|██████████| 4624615/4624615 [00:29<00:00, 154642.96 examples/s]\n",
      "Filter: 100%|██████████| 217724/217724 [00:08<00:00, 25326.42 examples/s]\n",
      "Filter: 100%|██████████| 6028884/6028884 [00:36<00:00, 163430.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Print a few examples from the loaded datasets\n",
    "for domain in domains:\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(\"Reviews:\")\n",
    "    print(dfs[domain]['reviews'][\"full\"][0])\n",
    "    print(\"Metadata:\")\n",
    "    print(dfs[domain]['metadata'][0])\n",
    "    print()\n",
    "\n",
    "# Print the dataset sizes\n",
    "for domain in domains:\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"Dataset size: {len(dfs[domain]['reviews']['full'])}\")\n",
    "\n",
    "\n",
    "\n",
    "# Count items with images\n",
    "for domain in domains:\n",
    "    items_with_images = dfs[domain]['metadata'].filter(lambda example: len(example['images']) > 0)\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"Items with images: {len(items_with_images)}\")\n",
    "    print(f\"Total items: {len(dfs[domain]['metadata'])}\")\n",
    "    print()\n",
    "\n",
    "# Filter reviews to keep only items with images\n",
    "for domain in domains:\n",
    "    items_with_images_ids = set(dfs[domain]['metadata'].filter(lambda example: len(example['images']) > 0)['parent_asin'])\n",
    "    dfs[domain]['reviews'] = dfs[domain]['reviews'].filter(lambda example: example['parent_asin'] in items_with_images_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 701528/701528 [00:09<00:00, 72830.73 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 112590/112590 [00:00<00:00, 173055.65 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 4624615/4624615 [01:05<00:00, 70257.41 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 137269/137269 [00:01<00:00, 87825.34 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 6028884/6028884 [01:25<00:00, 70342.90 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 217724/217724 [00:02<00:00, 89488.21 examples/s] \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_reviews(examples):\n",
    "    # Lowercase the text\n",
    "    examples[\"text\"] = [text.lower() for text in examples[\"text\"]]\n",
    "\n",
    "    # Remove punctuation and special characters using regex\n",
    "    examples[\"text\"] = [re.sub(r'[^\\w\\s]', \"\", text) for text in examples[\"text\"]]\n",
    "\n",
    "    # Tokenize the title\n",
    "    examples['title_tokens'] = [title.split() for title in examples['title']]\n",
    "\n",
    "    # Flatten the features list if it exists\n",
    "    if 'features' in examples:\n",
    "        examples['features'] = [' '.join(features) for features in examples['features']]\n",
    "\n",
    "    return examples\n",
    "\n",
    "def preprocess_metadata(examples):\n",
    "    # Lowercase the title\n",
    "    examples['title'] = [title.lower() for title in examples['title']]\n",
    "\n",
    "    # Remove punctuation and special characters from the title\n",
    "    examples['title'] = [re.sub(r'[^\\w\\s]', '', title) for title in examples['title']]\n",
    "\n",
    "    # Tokenize the title\n",
    "    examples['title_tokens'] = [title.split() for title in examples['title']]\n",
    "\n",
    "    # Flatten the features list\n",
    "    examples['features'] = [' '.join(features) for features in examples['features']]\n",
    "\n",
    "    return examples\n",
    "\n",
    "for domain in domains:\n",
    "    dfs[domain]['reviews'] = dfs[domain]['reviews'].map(preprocess_reviews, batched=True, num_proc=4)\n",
    "    dfs[domain]['metadata'] = dfs[domain]['metadata'].map(preprocess_metadata, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split the data into train, validation and test using temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "for domain in domains:\n",
    "    # Calculate sizes based on the new dictionary name\n",
    "    train_size = int(0.8 * len(dfs[domain]['reviews']['full']))\n",
    "    val_size = int(0.1 * len(dfs[domain]['reviews']['full']))\n",
    "    test_size = len(dfs[domain]['reviews']['full']) - train_size - val_size\n",
    "\n",
    "    # Perform the split\n",
    "    split_datasets = dfs[domain]['reviews']['full'].train_test_split(train_size=train_size, test_size=test_size + val_size, seed=42)\n",
    "\n",
    "    # Assign the train set and split the test set into validation and test\n",
    "    dfs[domain]['reviews'] = DatasetDict({\n",
    "        'train': split_datasets['train'],\n",
    "        'test': split_datasets['test'].train_test_split(train_size=val_size, seed=42)['test'],\n",
    "        'validation': split_datasets['test'].train_test_split(train_size=val_size, seed=42)['train']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: All_Beauty\n",
      "Train set size: 561222\n",
      "Validation set size: 70152\n",
      "Test set size: 70154\n",
      "\n",
      "Domain: Video_Games\n",
      "Train set size: 3699692\n",
      "Validation set size: 462461\n",
      "Test set size: 462462\n",
      "\n",
      "Domain: Baby_Products\n",
      "Train set size: 4823107\n",
      "Validation set size: 602888\n",
      "Test set size: 602889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for domain in domains:\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(\"Train set size:\", len(dfs[domain]['reviews']['train']))\n",
    "    print(\"Validation set size:\", len(dfs[domain]['reviews']['validation']))\n",
    "    print(\"Test set size:\", len(dfs[domain]['reviews']['test']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Baseline Models Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prepare the data for BERT4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 561222/561222 [00:26<00:00, 21201.81 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 70152/70152 [00:03<00:00, 20899.38 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 70154/70154 [00:03<00:00, 20676.01 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 3699692/3699692 [03:45<00:00, 16410.91 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 462461/462461 [00:28<00:00, 16191.67 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 462462/462462 [00:27<00:00, 16607.54 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 4823107/4823107 [04:05<00:00, 19629.69 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 602888/602888 [00:30<00:00, 19489.38 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 602889/602889 [00:30<00:00, 19463.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "def prepare_bert_input(examples, tokenizer, max_seq_length):\n",
    "    tokenized_inputs = tokenizer(examples['text'], max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    return {\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'],\n",
    "        'token_type_ids': tokenized_inputs['token_type_ids']\n",
    "    }\n",
    "\n",
    "max_seq_length = 128\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "for domain in domains:\n",
    "    dfs[domain]['reviews']['train'] = dfs[domain]['reviews']['train'].map(\n",
    "        lambda examples: prepare_bert_input(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=256,\n",
    "        num_proc=4\n",
    "    )\n",
    "    dfs[domain]['reviews']['validation'] = dfs[domain]['reviews']['validation'].map(\n",
    "        lambda examples: prepare_bert_input(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=256,\n",
    "        num_proc=4\n",
    "    )\n",
    "    dfs[domain]['reviews']['test'] = dfs[domain]['reviews']['test'].map(\n",
    "        lambda examples: prepare_bert_input(examples, tokenizer, max_seq_length),\n",
    "        batched=True,\n",
    "        batch_size=256,\n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define the BERT4Rec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert4rec_model(num_items):\n",
    "    config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_items)\n",
    "    model = BertForSequenceClassification(config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train and evaluate BERT4Rec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/8770 [01:26<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, torch\u001b[38;5;241m.\u001b[39mzeros(logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     51\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/amazon_diss/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/amazon_diss/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Check if MPS device is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS device.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"MPS device not found, using CPU.\")\n",
    "\n",
    "accumulation_steps = 4  # Adjust this value based on your memory constraints\n",
    "\n",
    "for domain in domains:\n",
    "    num_items = len(dfs[domain]['metadata'])\n",
    "    model = create_bert4rec_model(num_items).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(dfs[domain]['reviews']['train']['input_ids']),\n",
    "        torch.tensor(dfs[domain]['reviews']['train']['attention_mask']),\n",
    "        torch.tensor(dfs[domain]['reviews']['train']['token_type_ids'])\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    val_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(dfs[domain]['reviews']['validation']['input_ids']),\n",
    "        torch.tensor(dfs[domain]['reviews']['validation']['attention_mask']),\n",
    "        torch.tensor(dfs[domain]['reviews']['validation']['token_type_ids'])\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, pin_memory=True)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_accuracy = 0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\") as train_progress:\n",
    "            for i, batch in enumerate(train_progress):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, attention_mask, token_type_ids = batch\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits, torch.zeros(logits.shape[0], dtype=torch.long, device=device))\n",
    "                loss = loss / accumulation_steps\n",
    "                loss.backward()\n",
    "                \n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_accuracy += (logits.argmax(dim=1) == 0).float().mean().item()\n",
    "                \n",
    "                train_progress.set_postfix(loss=train_loss / (train_progress.n + 1), accuracy=train_accuracy / (train_progress.n + 1))\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, attention_mask, token_type_ids = batch\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                logits = outputs.logits\n",
    "                val_loss += loss_fn(logits, torch.zeros(logits.shape[0], dtype=torch.long, device=device)).item()\n",
    "                val_accuracy += (logits.argmax(dim=1) == 0).float().mean().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy /= len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    test_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(dfs[domain]['reviews']['test']['input_ids']),\n",
    "        torch.tensor(dfs[domain]['reviews']['test']['attention_mask']),\n",
    "        torch.tensor(dfs[domain]['reviews']['test']['token_type_ids'])\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, pin_memory=True)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, token_type_ids = batch\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs.logits\n",
    "            test_loss += loss_fn(logits, torch.zeros(logits.shape[0], dtype=torch.long, device=device)).item()\n",
    "            test_accuracy += (logits.argmax(dim=1) == 0).float().mean().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy /= len(test_loader)\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amazon_diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
